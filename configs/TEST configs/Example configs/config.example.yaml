# Channel defaults inform selection of data fields in pygwy/Gwyddion.
channel_defaults:
  # Used as hint to pick a modulus channel (name match, case-insensitive).
  modulus_family: "modulus"
  # Used as hint to pick a topography/height channel.
  topography_family: "height"

# Processing modes define how a TIFF is processed in pygwy.
# Philosophy: use Gwyddion/pygwy for core ops (leveling, filters, grains).
# Use Python-side helpers only when Gwyddion lacks the needed function.
modes:
  modulus_basic:
    channel_family: "modulus"      # pick modulus-like channel
    plane_level: true              # apply plane leveling
    median_size: 3                 # median filter size (odd int); set null to skip
    # Line correction using Gwyddion Align Rows (scan line artefacts)
    # line_correct:
    #   enable: true
    #   method: "median"            # median|polynomial|matching|... (defaults to median)
    #   direction: "horizontal"     # horizontal|vertical
    line_level_x: false            # legacy flag (maps to line_correct horizontal)
    line_level_y: false            # legacy flag (maps to line_correct vertical)
    clip_percentiles: null         # e.g., [5, 95] to clip extremes; null to skip
    # Optional mask stage (stats computed only on masked pixels).
    # mask:                         # Uncomment to enable masking for stats
    #   enable: true
    #   # Single mask (threshold example; edit threshold for your data):
    #   # method: "threshold"       # threshold|range|percentile
    #   # threshold: 0.0            # TODO: set your threshold
    #   # direction: "above"        # above|below
    #   # invert: false             # invert selection
    #   # on_empty: "error"         # error|warn|skip_row
    #   # OR multi-mask (ordered, combined with AND/OR):
    #   # combine: "and"            # and|or (order is respected)
    #   # steps:
    #   #   - { method: "threshold", threshold: 0.0, direction: "above" }  # TODO: set
    #   #   - { method: "percentile", percentiles: [5, 95] }
    # Optional stats filtering (Python-side) to exclude invalid/saturated pixels from avg/std.
    # This does not change the image; it only affects the computed stats.
    # stats_filter:
    #   min_value: 0.0              # exclude values below
    #   max_value: 1e12             # exclude values above
    #   exclude_nonpositive: true   # drop <= 0
    #   on_empty: "error"           # error|warn|skip_row if all pixels are excluded
    # Optional python_data_filtering: post-Gwyddion value filters + CSV exports.
    # python_data_filtering:
    #   enable: true
    #   export_raw_csv: true        # save (row,col,value,kept) before filters
    #   export_filtered_csv: true   # save after filters
    #   export_dir: "out/debug/pyfilter"
    #   on_empty: "warn"            # warn|skip_row|error when filters drop everything
    #   filters:
    #     - { type: "three_sigma", sigma: 3.0 }
    #     - { type: "chauvenet" }
    #     - { type: "min_max", min_value: 0.1, max_value: 50000.0 }
    metric_type: "modulus"         # core.metric_type value
    units: "kPa"
    expected_units: "kPa"          # optional validation target
    on_unit_mismatch: "error"      # "error" | "warn" | "skip_row"
    on_missing_units: "skip_row"   # "error" | "warn" | "skip_row"
  topography_flat:
    channel_family: "height"
    plane_level: true
    median_size: 3
    # line_correct:
    #   enable: true
    #   method: "median"            # defaults to median
    #   direction: "horizontal"
    line_level_x: false
    line_level_y: false
    clip_percentiles: null
    # mask:
    #   enable: true
    #   # method: "threshold"
    #   # threshold: 0.0            # TODO: set your threshold
    #   # direction: "above"
    #   # invert: false
    #   # on_empty: "error"
    #   # OR multi-mask:
    #   # combine: "and"
    #   # steps:
    #   #   - { method: "threshold", threshold: 0.0, direction: "above" }
    #   #   - { method: "percentile", percentiles: [5, 95] }
    metric_type: "topography_height"
    units: "nm"
    expected_units: "nm"
    on_unit_mismatch: "error"
  particle_count_basic:
    channel_family: "height"
    plane_level: false
    metric_type: "particle_count"
    threshold: null                # if null, use mean-based threshold
    # Optional review pack: emits PNG panels + a review.csv template under <output_dir>/review/
    # review_pack:
    #   enable: true
    #   image_format: "png"
    #   panel_basename_max_len: 120
    #   csv_name: "review.csv"
    units: "count"
    expected_units: "count"
    on_unit_mismatch: "error"
  raw_noop:
    channel_family: null           # first available channel
    plane_level: false
    metric_type: "raw"
    units: "a.u."

# Grid config for deriving row/col indices from filenames.
grid:
  # Regex with named groups `row` and `col` used to derive grid indices from the TIFF filename.
  #
  # The spec defines `grid.row_idx` and `grid.col_idx` as *zero-based* indices. SmartScan exports
  # commonly use 1-based indices (e.g. `...LOC_RC001001...`), so set `index_base: 1` to convert
  # `001 -> 0`, `002 -> 1`, etc.
  index_base: 1
  filename_regex: "r(?P<row>\\d+)_c(?P<col>\\d+)"

# Summarization options
summarize:
  recursive: false   # if true, search TIFFs recursively

# Debug/diagnostics options
debug:
  enable: false           # set true to enable debug logging/artifacts
  level: "info"           # info|debug
  artifacts: []           # ["mask", "leveled", "aligned", "filtered"]
  sample_limit: 3         # number of files to save artifacts for (<=0 means all)
  out_dir: "out/debug"    # where to write debug TIFFs
  log_fields: ["units", "mask_counts", "stats_counts"]
  raise_on_warn: false    # if true, treat WARN as errors
  echo_config: false      # if true, log the active mode/csv config snippet

# Optional input filtering (applied during manifest generation).
# Useful for excluding duplicate Forward/Backward or repeated scans without changing code.
# input_filters:
#   include_regex: ["Backward"]
#   exclude_regex: ["Forward"]

# CSV modes define column layout and mapping from ModeResultRecord keys.
csv_modes:
  default_scalar:
    columns:
      - { name: "source_file", from: "core.source_file" }
      # Optional filename-derived metadata (helps with Forward/Backward duplicates).
      - { name: "channel", from: "file.channel", default: "" }
      - { name: "direction", from: "file.direction", default: "" }
      - { name: "grid_id", from: "file.grid_id", default: "" }
      - { name: "date_code", from: "file.date_code", default: "" }
      - { name: "mode", from: "core.mode" }
      - { name: "metric_type", from: "core.metric_type" }
      - { name: "avg_value", from: "core.avg_value" }
      - { name: "std_value", from: "core.std_value" }
      - { name: "units", from: "core.units" }
      - { name: "nx", from: "core.nx" }
      - { name: "ny", from: "core.ny" }
      - { name: "row_idx", from: "grid.row_idx", default: -1 }
      - { name: "col_idx", from: "grid.col_idx", default: -1 }
    on_missing_field: "warn_null"

  particle_metrics:
    columns:
      - { name: "source_file", from: "core.source_file" }
      - { name: "mode", from: "core.mode" }
      - { name: "metric_type", from: "core.metric_type" }
      - { name: "count_total", from: "particle.count_total" }
      - { name: "count_density", from: "particle.count_density" }
      - { name: "mean_diam_px", from: "particle.mean_diameter_px" }
      - { name: "std_diam_px", from: "particle.std_diameter_px" }
      - { name: "nx", from: "core.nx" }
      - { name: "ny", from: "core.ny" }
      - { name: "row_idx", from: "grid.row_idx", default: -1 }
      - { name: "col_idx", from: "grid.col_idx", default: -1 }
    on_missing_field: "warn_null"

# Result schemas cast CSV rows to typed dicts for plotting/analysis.
result_schemas:
  default_scalar:
    from_csv_mode: "default_scalar"
    fields:
      - { field: "source_file", type: "string", column: "source_file" }
      - { field: "channel", type: "string", column: "channel" }
      - { field: "direction", type: "string", column: "direction" }
      - { field: "grid_id", type: "string", column: "grid_id" }
      - { field: "date_code", type: "string", column: "date_code" }
      - { field: "mode", type: "string", column: "mode" }
      - { field: "metric_type", type: "string", column: "metric_type" }
      - { field: "avg_value", type: "float", column: "avg_value" }
      - { field: "std_value", type: "float", column: "std_value" }
      - { field: "units", type: "string", column: "units" }
      - { field: "nx", type: "int", column: "nx" }
      - { field: "ny", type: "int", column: "ny" }
      - { field: "row_idx", type: "int", column: "row_idx" }
      - { field: "col_idx", type: "int", column: "col_idx" }

  particle_metrics:
    from_csv_mode: "particle_metrics"
    fields:
      - { field: "source_file", type: "string", column: "source_file" }
      - { field: "mode", type: "string", column: "mode" }
      - { field: "metric_type", type: "string", column: "metric_type" }
      - { field: "count_total", type: "int", column: "count_total" }
      - { field: "count_density", type: "float", column: "count_density" }
      - { field: "mean_diam_px", type: "float", column: "mean_diam_px" }
      - { field: "std_diam_px", type: "float", column: "std_diam_px" }
      - { field: "nx", type: "int", column: "nx" }
      - { field: "ny", type: "int", column: "ny" }
      - { field: "row_idx", type: "int", column: "row_idx" }
      - { field: "col_idx", type: "int", column: "col_idx" }

# Plotting modes map schema to recipe and styling.
plotting_modes:
  sample_bar_with_error:
    result_schema: "default_scalar"
    recipe: "sample_bar_with_error"
    title: "Sample Means"
    ylabel: "Value"
  histogram_avg:
    result_schema: "default_scalar"
    recipe: "histogram_avg"
    bins: 20
    title: "Average Distribution"
  scatter_avg_vs_std:
    result_schema: "default_scalar"
    recipe: "scatter_avg_vs_std"
    title: "Avg vs Std"
  mode_comparison_bar:
    result_schema: "default_scalar"
    recipe: "mode_comparison_bar"
    title: "Mode Comparison"
  heatmap_grid:
    result_schema: "default_scalar"
    recipe: "heatmap_grid"
    title: "Grid Heatmap"
    colorbar_label: "avg_value"
    # If multiple rows map to the same (row_idx, col_idx) cell (e.g. Forward/Backward duplicates),
    # choose how the heatmap aggregates:
    # - warn_mean (default): average duplicates
    # - warn_first / warn_last: take first/last value
    # - error: abort plotting
    duplicate_policy: "warn_mean"

# Aggregate modes define dataset-level aggregation of per-scan stats from summary.csv.
# These are Py3-side helpers that operate on the summary CSV output.
aggregate_modes:
  # Pooled mean/std across all pixels, grouped by mode+units (recommended default).
  dataset_pooled_by_mode_units:
    group_by: ["mode", "units"]
    value_col: "avg_value"
    std_col: "std_value"
    n_col: "n_valid"
    units_col: "units"
    allow_mixed_units: false
    out_relpath: "aggregates/dataset_pooled_by_mode_units.csv"

# Profiles provide convenient presets tying modes together.
profiles:
  modulus_grid:
    processing_mode: "modulus_basic"
    csv_mode: "default_scalar"
    # Optional post-processing on summary.csv:
    # aggregate_modes: ["dataset_pooled_by_mode_units"]

# File collection jobs: copy subsets out of mixed folders using fuzzy keyword matching.
# See `scripts/collect_files.py`.
file_collect_jobs:
  # Example: collect Modulus Backward scans (typos tolerated) into a dated folder.
  collect_modulus_backward:
    input_root: "C:/path/to/mixed_folder"
    recursive: true
    patterns: ["*.tif", "*.tiff"]
    include_keywords: ["Modulus", "Backward"]
    include_mode: "all"       # all keywords must match (fuzzy)
    exclude_keywords: ["Forward"]
    min_similarity: 0.85
    on_empty: "error"
    preserve_tree: false
    basename_max_len: 140
    path_max_len: 240
    extract:
      - { regex: "-(?P<date_code>\\d{6})-", map: { date_code: "date_code" } }
      - { regex: "(?P<direction>Forward|Backward)", map: { direction: "direction" } }
    output:
      out_root: "out/file_collect"
      run_name_template: "collect_{timestamp}_{job}"
      dest_subdir_template: "{date_code}/{direction}"
      rename_template: "{orig_stem}_{short_hash}{ext}"

# Jobs: one-stop configs that can run collect -> manifest -> pygwy -> plots -> aggregates.
jobs:
  example_modulus_job:
    input_root: "C:/path/to/mixed_folder"
    output_root: "out/jobs"
    run_name_template: "{job}_{timestamp}"
    profile: "modulus_grid"
    pattern: "*.tif;*.tiff"
    collect:
      enable: true
      job: "collect_modulus_backward"
      out_root: "out/file_collect"
    # Optional overrides:
    # processing_mode: "modulus_basic"
    # csv_mode: "default_scalar"
    # plotting_modes: ["sample_bar_with_error", "heatmap_grid"]
    # aggregate_modes: ["dataset_pooled_by_mode_units"]
    plotting_modes: ["sample_bar_with_error", "heatmap_grid"]

# Unit conversions (per mode). If detected units match a key, values are scaled
# by factor and units set to target.
unit_conversions:
  modulus_basic:
    kPa: { target: "kPa", factor: 1.0 }
    MPa: { target: "kPa", factor: 1000.0 }
    GPa: { target: "kPa", factor: 1e6 }
    Pa:  { target: "kPa", factor: 0.001 }
